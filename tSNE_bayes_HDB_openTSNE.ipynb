{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp4zsZvHz6Moc+sw0s3LeR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdlakic/t-SNE_binning/blob/main/tSNE_bayes_HDB_openTSNE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enter an explanation here."
      ],
      "metadata": {
        "id": "ffApYrtAO-FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Program settings\n",
        "#@markdown - Type the directory name where binned contigs will be saved:\n",
        "bins = \"bins\" #@param {type:\"string\"}\n",
        "#@markdown - Perplexity is the minimum number of expected DNA fragments in the same group.\n",
        "#@markdown - It has the biggest effect on the final outcome. Values in the 20-80 range are reasonable.\n",
        "#@markdown - Keep in mind that contigs are chopped into pieces, so the fragment number is greater than contig number.\n",
        "perplexity = 40 #@param {type:\"integer\"}\n",
        "#@markdown - Theta angle for Barnes-Hut t-SNE approximation ( https://arxiv.org/abs/1301.3342 )\n",
        "#@markdown - The range is [0 - 0.5] and values close to zero are more accurate but will result in very slow embedding.\n",
        "#@markdown - Leave it at default value of -1 for the script to decide the theta angle based on data size.\n",
        "theta_val = -1 #@param {type:\"number\"}\n",
        "#@markdown - Number of threads to use. Not sure if it has any effect in Colab, but it makes a difference for local runs.\n",
        "threads = 4 #@param [4,8,12] {type:\"raw\"}\n",
        "#@markdown - Dimensionality of t-SNE:\n",
        "dims = 2 #@param [2,3] {type:\"raw\"}\n",
        "#@markdown - Specify the shade of ellipses - high numbers make darker ellipses, while small are more transparet.\n",
        "image_alpha = 0.3 #@param {type:\"number\"}\n",
        "#@markdown - Type the plot name for final t-SNE clusters:\n",
        "tsne_plot = \"contigs_opentSNE_HDB_clusters.png\" #@param {type:\"string\"}\n",
        "#@markdown - What title do you want for the t-SNE plot?\n",
        "plot_title = \"t-SNE embedding\" #@param {type:\"string\"}\n",
        "#@markdown - Final CSV file with tSNE clusters:\n",
        "tsne_clusters = \"contigs_opentSNE_HDB_clusters.csv\" #@param {type:\"string\"}\n",
        "#@markdown - What type of initialization to use before running t-SNE?\n",
        "initial_embedding = \"spectral\" #@param [\"pca\", \"spectral\"]\n",
        "#@markdown - Use black background instead of white for the t-SNE plot?\n",
        "black_bkg = False #@param {type:\"boolean\"}\n",
        "#@markdown - Kmer size for nucleotide frequency calculation:\n",
        "kmer_size = 4 #@param [4,5] {type:\"raw\"}\n",
        "#@markdown - Cutoff size to retain contigs. Contigs smaller than this value are excluded from binning. This is also the overlap between contigs.\n",
        "low_cutoff = \"2k\" #@param [\"1k\", \"2k\", \"3k\", \"4k\", \"5k\"]\n",
        "#@markdown - Cutoff size for for chopping contigs:\n",
        "high_cutoff = \"10k\" #@param [\"10k\", \"20k\"]\n",
        "#@markdown - t-SNE metric:\n",
        "tsne_metric = \"cosine\" #@param [\"euclidean\",\"cosine\",\"manhattan\",\"cityblock\"]\n",
        "#@markdown - Acceptable fraction of unbinned data points (1 means keep all):\n",
        "noise_threshold = 0.2 #@param {type:\"number\"}\n",
        "#@markdown - Use Bayesian optimization to find clustering parameters? It is slower but more accurate.\n",
        "hyperopt = True #@param {type:\"boolean\"}\n",
        "#@markdown - Use FastICA to reduce dimensionality before t-SNE? Usually not necessary.\n",
        "ica = False #@param {type:\"boolean\"}\n",
        "#@markdown - Size of data points in t-SNE plot. Use smaller point size for large assemblies.\n",
        "point_size = 5 #@param [1,2,5,10] {type:\"raw\"}\n",
        "#@markdown - Type in a fixed random seed for reproducibility. Leave it at `-1` for a different seed each time.\n",
        "rand_seed = -1 #@param {type:\"integer\"}\n",
        "#@markdown - Do you want printed information along the way? \n",
        "verbose = True #@param {type:\"boolean\"}\n",
        "\n",
        "jobs = threads\n",
        "hdb_model = 'HDB_' + os.path.splitext(tsne_plot)[0] + '_model.joblib'\n",
        "if (rand_seed == -1):\n",
        "    rand_num = np.random.randint(1000000)\n",
        "else:\n",
        "    rand_num = rand_seed\n",
        "\n",
        "if low_cutoff == '1k':\n",
        "    lc_size = 1000\n",
        "    lc_abbrev = '1K'\n",
        "if low_cutoff == '2k':\n",
        "    lc_size = 2000\n",
        "    lc_abbrev = '2K'\n",
        "if low_cutoff == '3k':\n",
        "    lc_size = 3000\n",
        "    lc_abbrev = '3K'\n",
        "if low_cutoff == '4k':\n",
        "    lc_size = 4000\n",
        "    lc_abbrev = '4K'\n",
        "if low_cutoff == '5k':\n",
        "    lc_size = 5000\n",
        "    lc_abbrev = '5K'\n",
        "if high_cutoff == '10k':\n",
        "    hc_size = 10000\n",
        "    hc_abbrev = '10K'\n",
        "if high_cutoff == '20k':\n",
        "    hc_size = 20000\n",
        "    hc_abbrev = '20K'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uG3HDd64GZw1",
        "outputId": "dcb4c4a4-b15f-4ed9-f8e5-8a44f67cf8af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.1.0-1/Mambaforge-23.1.0-1-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:16\n",
            "ðŸ” Restarting kernel...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - seqkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boltons-23.0.0             |     pyhd8ed1ab_0         296 KB  conda-forge\n",
            "    conda-23.3.1               |  py310hff52083_0         941 KB  conda-forge\n",
            "    jsonpatch-1.32             |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    jsonpointer-2.0            |             py_0           9 KB  conda-forge\n",
            "    openssl-3.1.0              |       hd590300_3         2.5 MB  conda-forge\n",
            "    packaging-23.1             |     pyhd8ed1ab_0          45 KB  conda-forge\n",
            "    seqkit-2.4.0               |       h9ee0642_0         7.0 MB  bioconda\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        10.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boltons            conda-forge/noarch::boltons-23.0.0-pyhd8ed1ab_0 \n",
            "  jsonpatch          conda-forge/noarch::jsonpatch-1.32-pyhd8ed1ab_0 \n",
            "  jsonpointer        conda-forge/noarch::jsonpointer-2.0-py_0 \n",
            "  packaging          conda-forge/noarch::packaging-23.1-pyhd8ed1ab_0 \n",
            "  seqkit             bioconda/linux-64::seqkit-2.4.0-h9ee0642_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  conda                              23.1.0-py310hff52083_0 --> 23.3.1-py310hff52083_0 \n",
            "  openssl                                  3.1.0-h0b41bf4_0 --> 3.1.0-hd590300_3 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "packaging-23.1       | 45 KB     | :   0% 0/1 [00:00<?, ?it/s]\n",
            "jsonpatch-1.32       | 14 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "conda-23.3.1         | 941 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "boltons-23.0.0       | 296 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "jsonpointer-2.0      | 9 KB      | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.1.0        | 2.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "seqkit-2.4.0         | 7.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "jsonpatch-1.32       | 14 KB     | : 100% 1.0/1 [00:00<00:00,  5.27it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "jsonpointer-2.0      | 9 KB      | : 100% 1.0/1 [00:00<00:00,  5.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "packaging-23.1       | 45 KB     | :  36% 0.3554167208989544/1 [00:00<00:00,  1.58it/s]\n",
            "\n",
            "\n",
            "\n",
            "jsonpointer-2.0      | 9 KB      | : 100% 1.0/1 [00:00<00:00,  5.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "jsonpatch-1.32       | 14 KB     | : 100% 1.0/1 [00:00<00:00,  5.27it/s]\u001b[A\n",
            "\n",
            "conda-23.3.1         | 941 KB    | :   2% 0.01699687325585979/1 [00:00<00:13, 13.88s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.1.0        | 2.5 MB    | :   1% 0.006164165296246007/1 [00:00<00:39, 39.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "seqkit-2.4.0         | 7.0 MB    | :   0% 0.0022343799670598062/1 [00:00<01:52, 113.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "conda-23.3.1         | 941 KB    | :  24% 0.2379562255820371/1 [00:00<00:00,  1.15s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "packaging-23.1       | 45 KB     | : 100% 1.0/1 [00:00<00:00,  3.10it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "seqkit-2.4.0         | 7.0 MB    | :  26% 0.2591880761789375/1 [00:00<00:00,  1.07s/it]    \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "seqkit-2.4.0         | 7.0 MB    | :  62% 0.6166888709085064/1 [00:00<00:00,  1.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "boltons-23.0.0       | 296 KB    | : 100% 1.0/1 [00:00<00:00,  2.45it/s]                 \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "boltons-23.0.0       | 296 KB    | : 100% 1.0/1 [00:00<00:00,  2.45it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-3.1.0        | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  2.58it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "conda-23.3.1         | 941 KB    | : 100% 1.0/1 [00:00<00:00,  1.28it/s]               \u001b[A\u001b[A\n",
            "\n",
            "conda-23.3.1         | 941 KB    | : 100% 1.0/1 [00:00<00:00,  1.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "seqkit-2.4.0         | 7.0 MB    | : 100% 1.0/1 [00:02<00:00,  2.32s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: \\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "file                            format  type  num_seqs     sum_len  min_len  avg_len  max_len\n",
            "Perpetual_Spouter_A.contigs.fa  FASTA   DNA    133,783  94,317,186      200      705  449,239\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-51371976d888>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m#@param {type:\"boolean\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mnum_recycles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_recycles\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_recycles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mrecycle_early_stop_tolerance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecycle_early_stop_tolerance\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecycle_early_stop_tolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_msa\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_recycles' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for input_file in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=input_file, length=len(uploaded[input_file])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "AQLZ-JXIIwWZ",
        "outputId": "81b097da-5bc0-433a-be5a-b8f40c68aaf0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-efdb33a9-3e97-4ba7-9f62-d61517b08b18\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-efdb33a9-3e97-4ba7-9f62-d61517b08b18\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JGhtjOlFRS6",
        "outputId": "19421155-e2dc-4373-a7f4-56ffff3b382d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: checkm-genome in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: dendropy>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from checkm-genome) (4.6.0)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from checkm-genome) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from checkm-genome) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from checkm-genome) (67.7.2)\n",
            "Requirement already satisfied: pysam>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from checkm-genome) (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from checkm-genome) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->checkm-genome) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->checkm-genome) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->checkm-genome) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->checkm-genome) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->checkm-genome) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->checkm-genome) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->checkm-genome) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.1->checkm-genome) (2.8.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from pysam>=0.19.0->checkm-genome) (0.29.34)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.1->checkm-genome) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opentsne==0.7.1 in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from opentsne==0.7.1) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from opentsne==0.7.1) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from opentsne==0.7.1) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->opentsne==0.7.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->opentsne==0.7.1) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hdbscan==0.8.29 in /usr/local/lib/python3.10/dist-packages (0.8.29)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.29) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.29) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.29) (1.10.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.29) (0.29.34)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan==0.8.29) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->hdbscan==0.8.29) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "usage: cut_up_fasta.py\n",
            "       [-h]\n",
            "       [-c CHUNK_SIZE]\n",
            "       [-o OVERLAP_SIZE]\n",
            "       [-m]\n",
            "       [-b BEDFILE]\n",
            "       contigs\n",
            "       [contigs ...]\n",
            "\n",
            "Cut up fasta file in non-overlapping or overlapping parts of equal length.\n",
            "\n",
            "Optionally creates a BED-file where the cutup contigs are specified in terms\n",
            "of the original contigs. This can be used as input to concoct_coverage_table.py.\n",
            "\n",
            "positional arguments:\n",
            "  contigs\n",
            "    Fasta files\n",
            "    with\n",
            "    contigs\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  -c CHUNK_SIZE, --chunk_size CHUNK_SIZE\n",
            "    Chunk size\n",
            "  -o OVERLAP_SIZE, --overlap_size OVERLAP_SIZE\n",
            "    Overlap\n",
            "    size\n",
            "  -m, --merge_last\n",
            "    Concatenate\n",
            "    final part\n",
            "    to last\n",
            "    contig\n",
            "  -b BEDFILE, --bedfile BEDFILE\n",
            "    BEDfile to\n",
            "    be created\n",
            "    with exact\n",
            "    regions of\n",
            "    the\n",
            "    original\n",
            "    contigs cor\n",
            "    responding\n",
            "    to the\n",
            "    newly\n",
            "    created\n",
            "    contigs\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -c bioconda seqkit\n",
        "!pip install checkm-genome\n",
        "!pip install opentsne==0.7.1\n",
        "!pip install hdbscan==0.8.29\n",
        "!pip install seqtk\n",
        "!pip install wget\n",
        "import wget\n",
        "wget.download('https://github.com/BinPro/CONCOCT/raw/develop/scripts/cut_up_fasta.py')\n",
        "# !python cut_up_fasta.py -h\n",
        "import os\n",
        "import shutil\n",
        "import itertools\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns\n",
        "sns.set(style='white', color_codes=True)\n",
        "import numpy as np\n",
        "np.random.seed()\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.ticker\n",
        "import matplotlib.animation as animation\n",
        "from openTSNE import TSNE\n",
        "import hdbscan\n",
        "from skimage.measure import EllipseModel\n",
        "from matplotlib.patches import Ellipse\n",
        "from scipy import linalg\n",
        "import joblib\n",
        "from sklearn.metrics import silhouette_score\n",
        "from termcolor import colored\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.decomposition import FastICA\n",
        "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 50000)\n",
        "pd.set_option('display.max_columns', 50000)\n",
        "pd.set_option('max_colwidth', 1000)\n",
        "\n",
        "search_space = {\n",
        "    'min_cluster_size': hp.quniform('x_min_cluster_size', 10, 75, 1),\n",
        "    'min_samples': hp.quniform('x_min_samples', 5, 60, 1),\n",
        "    'cluster_selection_epsilon': hp.uniform('x_cluster_selection_epsilon', 0, 0.5)\n",
        "               }\n",
        "\n",
        "trials = Trials()"
      ],
      "metadata": {
        "id": "psy6U1sDFgel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperspace(search_space):\n",
        "\n",
        "    global jobs\n",
        "    global min_cluster_size_list\n",
        "    global min_samples_list\n",
        "    global epsilon_list\n",
        "    global noise_list\n",
        "    global silhouette_list\n",
        "    global fraction_list\n",
        "    global sscore_best\n",
        "    global failed_score_best\n",
        "    global noise_best\n",
        "    global cluster_list\n",
        "    global noise_threshold\n",
        "\n",
        "    print('\\n Current HDBSCAN parameters: min_cluster_size = %d  min_samples = %d  cluster_selection_epsilon = %.5f' % (search_space['min_cluster_size'], search_space['min_samples'], search_space['cluster_selection_epsilon']) )\n",
        "    min_cluster_size_list.append(int(search_space['min_cluster_size']))\n",
        "    min_samples_list.append(int(search_space['min_samples']))\n",
        "    epsilon_list.append(search_space['cluster_selection_epsilon'])\n",
        "    hdbscanner = hdbscan.HDBSCAN(min_cluster_size=int(search_space['min_cluster_size']), min_samples=int(search_space['min_samples']), cluster_selection_epsilon=search_space['cluster_selection_epsilon'], core_dist_n_jobs=jobs, prediction_data=True)\n",
        "\n",
        "    tmp = train\n",
        "    tmp['clabels'] = hdbscanner.fit_predict(train[col_names].values)\n",
        "    noise = tmp.loc[tmp.clabels == -1]\n",
        "    noise_list.append(len(noise))\n",
        "    cluster_list.append(len(np.unique(tmp.clabels))-1)\n",
        "    fraction_list.append(len(noise)/float(total_points))\n",
        "    if len(noise) < noise_best:\n",
        "        noise_best = len(noise)\n",
        "        noise_str = colored('%.3f %%' % (100*(len(noise)/float(total_points))), 'white', 'on_red')\n",
        "    else:\n",
        "        noise_str = str('%.3f %%' % (100*(len(noise)/float(total_points))))\n",
        "    clean = tmp.loc[tmp.clabels != -1]\n",
        "    clean.reset_index(drop=True, inplace=True)\n",
        "    clean_data = clean.drop(['clabels'], axis=1).values\n",
        "    sscore = silhouette_score(clean_data, clean['clabels'].values, metric='euclidean')\n",
        "    if (noise_threshold>(len(noise)/float(total_points))):\n",
        "        silhouette_list.append(sscore)\n",
        "    else:\n",
        "        silhouette_list.append(0.1)\n",
        "    if (sscore > sscore_best) and (noise_threshold>(len(noise)/float(total_points))):\n",
        "        sscore_best = sscore\n",
        "        sscore_str = colored('%.5f' % sscore, 'white', 'on_blue')\n",
        "    elif (sscore > sscore_best) and (sscore > failed_score_best) and (noise_threshold<(len(noise)/float(total_points))):\n",
        "        failed_score_best = sscore\n",
        "        sscore_str = colored('%.5f' % sscore, 'red', 'on_yellow')\n",
        "    else:\n",
        "        sscore_str = str('%.5f' % sscore)\n",
        "    if verbose:\n",
        "        print(' silhouette_score: %s  noise: %s  clusters: %d' % (sscore_str, noise_str, np.unique(clean['clabels'].values).shape[0]) )\n",
        "\n",
        "    std_score = 0.01\n",
        "    if (noise_threshold>(len(noise)/float(total_points))):\n",
        "        val_score = -1.0 * sscore\n",
        "    else:\n",
        "        val_score = -0.1\n",
        "\n",
        "    return {'loss': val_score, 'loss_variance': std_score, 'status': STATUS_OK }"
      ],
      "metadata": {
        "id": "14My1j4_Fj5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EllipsoidTool:\n",
        "    \"\"\"Some stuff for playing with ellipsoids\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def getMinVolEllipse(self, P=None, tolerance=0.001):\n",
        "        \"\"\" Find the minimum volume ellipsoid which holds all the points\n",
        "\n",
        "        Based on work by Nima Moshtagh\n",
        "        http://www.mathworks.com/matlabcentral/fileexchange/9542\n",
        "        and also by looking at:\n",
        "        http://cctbx.sourceforge.net/current/python/scitbx.math.minimum_covering_ellipsoid.html\n",
        "        Which is based on the first reference anyway!\n",
        "\n",
        "        Here, P is a numpy array of N dimensional points like this:\n",
        "        P = [[x,y,z,...], <-- one point per line\n",
        "             [x,y,z,...],\n",
        "             [x,y,z,...]]\n",
        "\n",
        "        Returns:\n",
        "        (center, radii, rotation)\n",
        "\n",
        "        \"\"\"\n",
        "        (N, d) = np.shape(P)\n",
        "        d = float(d)\n",
        "\n",
        "        # Q will be our working array\n",
        "        Q = np.vstack([np.copy(P.T), np.ones(N)])\n",
        "        QT = Q.T\n",
        "\n",
        "        # initializations\n",
        "        err = 1.0 + tolerance\n",
        "        u = (1.0 / N) * np.ones(N)\n",
        "\n",
        "        # Khachiyan Algorithm\n",
        "        while err > tolerance:\n",
        "            V = np.dot(Q, np.dot(np.diag(u), QT))\n",
        "            M = np.diag(np.dot(QT, np.dot(\n",
        "                linalg.inv(V), Q)))  # M the diagonal vector of an NxN matrix\n",
        "            j = np.argmax(M)\n",
        "            maximum = M[j]\n",
        "            step_size = (maximum - d - 1.0) / ((d + 1.0) * (maximum - 1.0))\n",
        "            new_u = (1.0 - step_size) * u\n",
        "            new_u[j] += step_size\n",
        "            err = np.linalg.norm(new_u - u)\n",
        "            u = new_u\n",
        "\n",
        "        # center of the ellipse\n",
        "        center = np.dot(P.T, u)\n",
        "\n",
        "        # the A matrix for the ellipse\n",
        "        A = linalg.inv(\n",
        "            np.dot(P.T, np.dot(np.diag(u), P)) -\n",
        "            np.array([[a * b for b in center] for a in center])) / d\n",
        "\n",
        "        # Get the values we'd like to return\n",
        "        U, s, rotation = linalg.svd(A)\n",
        "        radii = 1.0 / np.sqrt(s)\n",
        "\n",
        "        return (center, radii, rotation)\n",
        "\n",
        "    def getEllipsoidVolume(self, radii):\n",
        "        \"\"\"Calculate the volume of the blob\"\"\"\n",
        "        return 4. / 3. * np.pi * radii[0] * radii[1] * radii[2]\n",
        "\n",
        "    def plotEllipsoid(self,\n",
        "                      center,\n",
        "                      radii,\n",
        "                      rotation,\n",
        "                      ax=None,\n",
        "                      plotAxes=False,\n",
        "                      cageColor='b',\n",
        "                      cageAlpha=0.2):\n",
        "        \"\"\"Plot an ellipsoid\"\"\"\n",
        "        make_ax = ax == None\n",
        "        if make_ax:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            ax.set_aspect('equal')\n",
        "\n",
        "        u = np.linspace(0.0, 2.0 * np.pi, 100)\n",
        "        v = np.linspace(0.0, np.pi, 100)\n",
        "\n",
        "        # cartesian coordinates that correspond to the spherical angles:\n",
        "        x = radii[0] * np.outer(np.cos(u), np.sin(v))\n",
        "        y = radii[1] * np.outer(np.sin(u), np.sin(v))\n",
        "        z = radii[2] * np.outer(np.ones_like(u), np.cos(v))\n",
        "        # rotate accordingly\n",
        "        for i in range(len(x)):\n",
        "            for j in range(len(x)):\n",
        "                [x[i, j], y[i, j], z[i, j]\n",
        "                 ] = np.dot([x[i, j], y[i, j], z[i, j]], rotation) + center\n",
        "\n",
        "        if plotAxes:\n",
        "            # make some purdy axes\n",
        "            axes = np.array([[radii[0], 0.0, 0.0], [0.0, radii[1], 0.0],\n",
        "                             [0.0, 0.0, radii[2]]])\n",
        "            # rotate accordingly\n",
        "            for i in range(len(axes)):\n",
        "                axes[i] = np.dot(axes[i], rotation)\n",
        "\n",
        "            # plot axes\n",
        "            for p in axes:\n",
        "                X3 = np.linspace(-p[0], p[0], 100) + center[0]\n",
        "                Y3 = np.linspace(-p[1], p[1], 100) + center[1]\n",
        "                Z3 = np.linspace(-p[2], p[2], 100) + center[2]\n",
        "                ax.plot(X3, Y3, Z3, color=cageColor)\n",
        "\n",
        "        # plot ellipsoid\n",
        "        # ax.plot_wireframe(x, y, z,  rstride=4, cstride=4, color=cageColor, alpha=cageAlpha)\n",
        "        ax.plot_wireframe(x,\n",
        "                          y,\n",
        "                          z,\n",
        "                          rstride=2,\n",
        "                          cstride=2,\n",
        "                          color=cageColor,\n",
        "                          alpha=cageAlpha)\n",
        "\n",
        "        if make_ax:\n",
        "            plt.show()\n",
        "            plt.close(fig)\n",
        "            del fig"
      ],
      "metadata": {
        "id": "P5tbF8xaF9LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def timer(start_time=None):\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(),\n",
        "                                 3600)\n",
        "        tmin, tsec = divmod(temp_sec, 60)\n",
        "        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n",
        "              (thour, tmin, round(tsec, 2)))"
      ],
      "metadata": {
        "id": "jty6A9QtGGZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_tsne(data=None,\n",
        "              n_neighbors=15,\n",
        "              min_dist=0.1,\n",
        "              n_components=2,\n",
        "              metric='euclidean',\n",
        "              title=''):\n",
        "    print('n_neighbors=%d   min_dist=%.2f' % (n_neighbors, min_dist))\n",
        "    fit = tsne.tSNE(n_neighbors=n_neighbors,\n",
        "                    min_dist=min_dist,\n",
        "                    n_components=n_components,\n",
        "                    metric=metric)\n",
        "    start_time = timer(None)\n",
        "    u = fit.fit_transform(data)\n",
        "    timer(start_time)\n",
        "\n",
        "    hdbscanner = hdbscan.HDBSCAN(min_cluster_size=20)\n",
        "    labels = hdbscanner.fit_predict(u)\n",
        "    print('Clusters %d' % np.max(labels))\n",
        "\n",
        "    fig = plt.figure(figsize=(7, 6))\n",
        "    if n_components == 1:\n",
        "        ax = fig.add_subplot(111)\n",
        "        cb = ax.scatter(u[:, 0], range(len(u)), cmap=cm, c=labels, s=2)\n",
        "        plt.colorbar(cb)\n",
        "    if n_components == 2:\n",
        "        ax = fig.add_subplot(111)\n",
        "        cb = ax.scatter(u[:, 0], u[:, 1], cmap=cm, c=labels, s=2)\n",
        "        plt.colorbar(cb)\n",
        "    if n_components == 3:\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        cb = ax.scatter(u[:, 0], u[:, 1], u[:, 2], cmap=cm, c=labels, s=2)\n",
        "        plt.colorbar(cb)\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_results(X, Y_, nclus, index, title):\n",
        "\n",
        "    unique_labels = np.arange(nclus).astype(np.int)\n",
        "    splot = plt.subplot(1, 2, index)\n",
        "    splot.tick_params(axis='both', which='major', labelsize=12)\n",
        "    splot.tick_params(axis='both', which='minor', labelsize=12)\n",
        "    for i, (zz, color) in enumerate(zip(unique_labels, color_iter_one)):\n",
        "        if not np.any(Y_ == i):\n",
        "            continue\n",
        "        plt.scatter(X[Y_ == zz, 0],\n",
        "                    X[Y_ == zz, 1],\n",
        "                    marker='.',\n",
        "                    s=point_size,\n",
        "                    c=color)\n",
        "\n",
        "    if np.min(Y_) == -1:\n",
        "        plt.scatter(X[Y_ == -1, 0],\n",
        "                    X[Y_ == -1, 1],\n",
        "                    marker='.',\n",
        "                    s=point_size,\n",
        "                    c='k')\n",
        "\n",
        "    ell = EllipseModel()\n",
        "    for zz in unique_labels:\n",
        "        dp = X[Y_ == zz]\n",
        "        in_out = EllipticEnvelope(contamination=0.05).fit_predict(dp)\n",
        "        a_points = dp[in_out == 1]\n",
        "        ell_status = ell.estimate(a_points)\n",
        "        if ell_status:\n",
        "            xc, yc, a, b, theta = ell.params\n",
        "            ell_patch = Ellipse((xc, yc),\n",
        "                                3 * a,\n",
        "                                3 * b,\n",
        "                                theta * 180 / np.pi,\n",
        "                                edgecolor='red',\n",
        "                                facecolor='none')\n",
        "            splot.add_patch(ell_patch)\n",
        "            plt.text(xc, yc, str(zz), fontsize=11)\n",
        "\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.xlabel('Dimension 1')\n",
        "    plt.ylabel('Dimension 2')\n",
        "    plt.title(title, fontsize=15)\n",
        "\n",
        "def ani_update(embedding, ax, pathcol):\n",
        "    # Update point positions\n",
        "    pathcol.set_offsets(embedding)\n",
        "\n",
        "    # Adjust x/y limits so all the points are visible\n",
        "    ax.set_xlim(np.min(embedding[:, 0])-5, np.max(embedding[:, 0])+5)\n",
        "    ax.set_ylim(np.min(embedding[:, 1])-5, np.max(embedding[:, 1])+5)\n",
        "\n",
        "    return [pathcol]"
      ],
      "metadata": {
        "id": "ReirbEeFGKS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.access(input_file, os.R_OK):\n",
        "\n",
        "    if not os.path.exists(bins):\n",
        "        os.makedirs(bins)  # to create a directory recursively\n",
        "    else:\n",
        "        print(\n",
        "            '\\n !! The directory you chose already exists, and its contents will be cleared !!'\n",
        "        )\n",
        "        for root, dirs, files in os.walk(bins):\n",
        "            for name in files:\n",
        "                os.remove(os.path.join(root, name))\n",
        "            for name in dirs:\n",
        "                shutil.rmtree(os.path.join(root, name))\n",
        "\n",
        "    if verbose:\n",
        "        print('\\n Splitting DNA sequences into %s pieces ...' % hc_abbrev)\n",
        "    os.system('python cut_up_fasta.py -c %d -o %d -m %s > contigs_%s.fasta' %\n",
        "              (hc_size, lc_size, input_file, hc_abbrev))\n",
        "    if verbose:\n",
        "        print('\\n Removing contigs smaller than %s ...' % lc_abbrev)\n",
        "    os.system('seqtk seq -L %d contigs_%s.fasta > contigs_%s_%s.fasta' %\n",
        "              (lc_size, hc_abbrev, hc_abbrev, lc_abbrev))\n",
        "    if verbose:\n",
        "        print('\\n Calculating %d-mer frequencies ...' % kmer_size)\n",
        "    os.system(\n",
        "        'checkm tetra -t 6 contigs_%s_%s.fasta contigs_%s_%s.%dmer.tab > /dev/null'\n",
        "        % (hc_abbrev, lc_abbrev, hc_abbrev, lc_abbrev, kmer_size))\n",
        "    os.system('cp contigs_%s_%s.%dmer.tab contigs_%s_%s.%dmer.csv' %\n",
        "              (hc_abbrev, lc_abbrev, kmer_size, hc_abbrev, lc_abbrev,\n",
        "               kmer_size))\n",
        "    os.system('perl -pi -e \"s/\\t/\\,/g\" contigs_%s_%s.%dmer.csv' %\n",
        "              (hc_abbrev, lc_abbrev, kmer_size))\n",
        "    os.system(\n",
        "        'awk -F , \"{print \\$1}\" contigs_%s_%s.%dmer.csv > contigs_%s_%s.%dmer_index.csv'\n",
        "        % (hc_abbrev, lc_abbrev, kmer_size, hc_abbrev, lc_abbrev,\n",
        "           kmer_size))\n",
        "    os.system(\n",
        "        'delete_cols.py contigs_%s_%s.%dmer.csv contigs_%s_%s.%dmer_noindex.csv 0'\n",
        "        % (hc_abbrev, lc_abbrev, kmer_size, hc_abbrev, lc_abbrev,\n",
        "           kmer_size))\n",
        "\n",
        "    train_data = pd.read_csv('contigs_%s_%s.%dmer_noindex.csv' %\n",
        "                             (hc_abbrev, lc_abbrev, kmer_size))\n",
        "    train_data = train_data.values\n",
        "    if (theta_val == -1.0):\n",
        "        if train_data.shape[0] <= 10000:\n",
        "            theta_val = 0.1\n",
        "            early_exag = 1000\n",
        "            iters = 3000\n",
        "            psize = 2\n",
        "        if (train_data.shape[0] > 10000) and (train_data.shape[0] <= 20000):\n",
        "            theta_val = 0.2\n",
        "            early_exag = 1500\n",
        "            iters = 4000\n",
        "            psize = 1\n",
        "        if train_data.shape[0] > 20000:\n",
        "            theta_val = 0.5\n",
        "            early_exag = 2000\n",
        "            iters = 5000\n",
        "            psize = 1\n",
        "    else:\n",
        "        if train_data.shape[0] <= 10000:\n",
        "            theta_val = theta_val\n",
        "            early_exag = 1000\n",
        "            iters = 3000\n",
        "            psize = 2\n",
        "        if (train_data.shape[0] > 10000) and (train_data.shape[0] <= 20000):\n",
        "            theta_val = theta_val\n",
        "            early_exag = 1500\n",
        "            iters = 4000\n",
        "            psize = 1\n",
        "        if train_data.shape[0] > 20000:\n",
        "            theta_val = theta_val\n",
        "            early_exag = 2000\n",
        "            iters = 5000\n",
        "            psize = 1\n",
        "\n",
        "    if dims == 2:\n",
        "        embeddings = []\n",
        "        if ica:\n",
        "            ICA = FastICA(n_components=8, whiten=False, max_iter=20000, tol=0.0001)\n",
        "            train_data_ICA = ICA.fit_transform(train_data)\n",
        "        tsne = TSNE(perplexity=perplexity,\n",
        "                    metric=tsne_metric,\n",
        "                    n_jobs=jobs,\n",
        "                    learning_rate='auto',\n",
        "                    negative_gradient_method='bh',\n",
        "                    theta=theta_val,\n",
        "#                    initialization='spectral',\n",
        "                    initialization=initial_embedding,\n",
        "                    early_exaggeration_iter=early_exag,\n",
        "                    early_exaggeration=18.0,\n",
        "                    n_iter=iters,\n",
        "                    # The embedding will be appended to the list we defined above, make sure we copy the\n",
        "                    # embedding, otherwise the same object reference will be stored for every iteration\n",
        "                    callbacks=lambda it, err, emb: embeddings.append(np.array(emb)),\n",
        "                    # This should be done on every iteration\n",
        "                    callbacks_every_iters=10,\n",
        "                    verbose=verbose)\n",
        "        start_time = timer(None)\n",
        "        if ica:\n",
        "            train = tsne.fit(train_data_ICA)\n",
        "        else:\n",
        "            train = tsne.fit(train_data)\n",
        "\n",
        "        embeddings = embeddings[(int(early_exag/10)-2):]\n",
        "        fig = plt.figure(figsize=(6, 6))\n",
        "        ax = fig.add_axes([0, 0, 1, 1])\n",
        "        ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.gca().set_aspect('equal', adjustable='box')\n",
        "        ax.set_xticks([]), ax.set_yticks([])\n",
        "        pathcol = ax.scatter(embeddings[0][:, 0], embeddings[0][:, 1], c='r', s=psize, rasterized=True)\n",
        "        anim = animation.FuncAnimation(fig, ani_update, fargs=(ax, pathcol), interval=100, frames=embeddings, blit=True)\n",
        "        ani_name = tsne_plot.rsplit('.', 1)[0] + '.gif'\n",
        "        mp4_name = tsne_plot.rsplit('.', 1)[0] + '.mp4'\n",
        "        anim.save(ani_name, dpi=100, writer=animation.PillowWriter(fps=15))\n",
        "        anim.save(mp4_name, dpi=100, fps=15)\n",
        "        plt.close()\n",
        "\n",
        "        total_points = train.shape[0]\n",
        "        train = pd.DataFrame(train)\n",
        "        timer(start_time)\n",
        "    if dims == 3:\n",
        "        os.system(\n",
        "            'TSNEGenerator -i contigs_%s_%s.%dmer_noindex.csv -o data.dat -d 3 -p 20 -r %d -m 6000 > /dev/null'\n",
        "            % (hc_abbrev, lc_abbrev, kmer_size, rand_num))\n",
        "        print('\\n Starting tSNE calculation. This may take a while ...')\n",
        "        start_time = timer(None)\n",
        "        os.system('bh_tsne_exag')\n",
        "        timer(start_time)\n",
        "        os.system('TSNEReader -i result.dat -o tmp.csv > /dev/null')\n",
        "        train = pd.read_csv('tmp.csv', header=None)\n",
        "        total_points = train.shape[0]\n",
        "    if dims == 2:\n",
        "        train.columns = ['tSNE_X', 'tSNE_Y']\n",
        "    else:\n",
        "        train.columns = ['tSNE_X', 'tSNE_Y', 'tSNE_Z']\n",
        "    train.to_csv('contigs_hdbscan_%s_%s_%dD_%dmer_opentSNE.csv' %\n",
        "                 (hc_abbrev, lc_abbrev, dims, kmer_size),\n",
        "                 index=False)\n",
        "\n",
        "    col_names = train.columns.values\n",
        "    col_names_X = train.columns[0]\n",
        "    col_names_Y = train.columns[1]\n",
        "    if dims == 3:\n",
        "        col_names_Z = train.columns[2]\n",
        "\n",
        "    if dims == 2:\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.gca().set_aspect('equal', adjustable='box')\n",
        "        plt.scatter(train[col_names_X],\n",
        "                    train[col_names_Y],\n",
        "                    marker='.',\n",
        "                    s=point_size,\n",
        "                    color='r')\n",
        "        plt.xlabel('Dimension 1')\n",
        "        plt.ylabel('Dimension 2')\n",
        "        plt.title('Initial tSNE embedding before clustering')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('scatter_' + tsne_plot, dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    if dims == 3:\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.set_aspect('equal')\n",
        "        ax.set_xlabel('Dimension 1')\n",
        "        ax.set_ylabel('Dimension 2')\n",
        "        ax.set_zlabel('Dimension 3')\n",
        "        ax.scatter(train[col_names_X],\n",
        "                   train[col_names_Y],\n",
        "                   train[col_names_Z],\n",
        "                   marker='.',\n",
        "                   s=point_size,\n",
        "                   c='r')\n",
        "\n",
        "        plt.title('Initial tSNE embedding before clustering')\n",
        "        # plt.legend(loc='best')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('scatter_' + tsne_plot, dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    # Fit a Dirichlet process Gaussian mixture using given number of components\n",
        "    print(\n",
        "        '\\n Starting HDBSCAN clustering. This may take a while ...\\n')\n",
        "    start_time = timer(None)\n",
        "\n",
        "    min_cluster_size_list = []\n",
        "    min_samples_list = []\n",
        "    epsilon_list = []\n",
        "    noise_list = []\n",
        "    silhouette_list = []\n",
        "    fraction_list = []\n",
        "    sscore_best = -1\n",
        "    failed_score_best = -1\n",
        "    noise_best = total_points\n",
        "    cluster_list = []\n",
        "    noise_threshold = noise_threshold\n",
        "    if not hyperopt:\n",
        "        for kk in np.arange(10,41,2):\n",
        "            for jj in np.arange(10,21):\n",
        "                print(' min_cluster_size: %d   min_samples: %d' % (kk, jj) )\n",
        "                min_cluster_size_list.append(int(kk))\n",
        "                min_samples_list.append(int(jj))\n",
        "                epsilon_list.append(0.)\n",
        "                hdbscanner = hdbscan.HDBSCAN(min_cluster_size=int(kk), min_samples=int(jj), core_dist_n_jobs=jobs, prediction_data=True)\n",
        "                # hdbscanner = hdbscan.HDBSCAN(min_cluster_size=kk, core_dist_n_jobs=jobs, prediction_data=True)\n",
        "                tmp = train\n",
        "                tmp['clabels'] = hdbscanner.fit_predict(train[col_names].values)\n",
        "                noise = tmp.loc[tmp.clabels == -1]\n",
        "                noise_list.append(len(noise))\n",
        "                cluster_list.append(len(np.unique(tmp.clabels))-1)\n",
        "                fraction_list.append(len(noise)/float(total_points))\n",
        "                if len(noise) < noise_best:\n",
        "                    noise_best = len(noise)\n",
        "                    noise_str = colored('%.3f %%' % (100*(len(noise)/float(total_points))), 'white', 'on_red')\n",
        "                else:\n",
        "                    noise_str = str('%.3f %%' % (100*(len(noise)/float(total_points))))\n",
        "                clean = tmp.loc[tmp.clabels != -1]\n",
        "                clean.reset_index(drop=True, inplace=True)\n",
        "                clean_data = clean.drop(['clabels'], axis=1).values\n",
        "                sscore = silhouette_score(clean_data, clean['clabels'].values, metric='euclidean')\n",
        "                silhouette_list.append(sscore)\n",
        "                if sscore > sscore_best:\n",
        "                    sscore_best = sscore\n",
        "                    sscore_str = colored('%.5f' % sscore, 'white', 'on_blue')\n",
        "                else:\n",
        "                    sscore_str = str('%.5f' % sscore)\n",
        "                if verbose:\n",
        "                    print(' silhouette_score: %s  noise: %s  clusters: %d' % (sscore_str, noise_str, np.unique(clean['clabels'].values).shape[0]) )\n",
        "\n",
        "    if hyperopt:\n",
        "        best = fmin(\n",
        "                    fn=hyperspace,  # \"Loss\" function to minimize\n",
        "                    space=search_space,  # Hyperparameter space\n",
        "#                    algo=anneal.suggest,  # Simulated annealing\n",
        "                    algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
        "                    trials=trials,\n",
        "                    max_evals=200,  # Perform 200 trials\n",
        "#                    max_evals=10,  # Perform 10 trials\n",
        "                   )\n",
        "        print('-'*70)\n",
        "        print('Best HDBSCAN parameters:')\n",
        "        print(str(best))\n",
        "\n",
        "    timer(start_time)\n",
        "\n",
        "    df = pd.DataFrame(min_cluster_size_list, columns=['min_cluster_size'])\n",
        "    df['min_samples'] = min_samples_list\n",
        "    df['epsilon'] = epsilon_list\n",
        "    df['clusters'] = cluster_list\n",
        "#    df['noise'] = noise_list\n",
        "    df['noise'] = fraction_list\n",
        "    df['noise'] = 100*df['noise'].values\n",
        "    df['silhouette_score'] = silhouette_list\n",
        "    df.sort_values('silhouette_score', ascending=False, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    arr = df.values\n",
        "    print(df)\n",
        "\n",
        "    nclus = arr[0][3]\n",
        "    if not hyperopt:\n",
        "        hdbscanner = hdbscan.HDBSCAN(min_cluster_size=int(arr[0][0]), min_samples=int(arr[0][1]), core_dist_n_jobs=jobs, prediction_data=True)\n",
        "    if hyperopt:\n",
        "        hdbscanner = hdbscan.HDBSCAN(min_cluster_size=int(arr[0][0]), min_samples=int(arr[0][1]), cluster_selection_epsilon=float(arr[0][2]), core_dist_n_jobs=jobs, prediction_data=True)\n",
        "    clabels = hdbscanner.fit_predict(train[col_names].values)\n",
        "    joblib.dump(hdbscanner, hdb_model)\n",
        "\n",
        "    clusters = pd.DataFrame(clabels.astype(np.int))\n",
        "    train['cluster_labels'] = clabels\n",
        "    train_new = train.loc[train.cluster_labels != -1]\n",
        "    train_new.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if dims == 2:\n",
        "        plt.figure(figsize=(22, 10))\n",
        "        if black_bkg:\n",
        "            plt.style.use('dark_background')\n",
        "        color_iter = itertools.cycle([\n",
        "            'orchid', 'cyan', 'violet', 'lawngreen', 'cornflowerblue', 'tomato',\n",
        "            'gold', 'deeppink', 'lime', 'darkorange', 'olive', 'lightcoral',\n",
        "            'tan', 'sandybrown', 'palegreen', 'lightslategrey',\n",
        "            'teal', 'red', 'dodgerblue', 'darkslategrey', 'mediumslateblue',\n",
        "            'orangered', 'springgreen', 'mediumaquamarine', 'darkgrey', 'firebrick',\n",
        "            'salmon', 'darkkhaki', 'yellowgreen', 'forestgreen', 'cadetblue',\n",
        "            'plum', 'seagreen', 'royalblue', 'goldenrod', 'slateblue', 'fuchsia',\n",
        "            'khaki', 'chocolate', 'hotpink', 'moccasin', 'crimson', 'peachpuff'\n",
        "        ])\n",
        "        color_iter_one = itertools.islice(color_iter, 1, None)\n",
        "        plot_results(train[col_names].values, clabels, nclus, 1, plot_title)\n",
        "        color_iter = itertools.cycle([\n",
        "            'orchid', 'cyan', 'violet', 'lawngreen', 'cornflowerblue', 'tomato',\n",
        "            'gold', 'deeppink', 'lime', 'darkorange', 'olive', 'lightcoral',\n",
        "            'tan', 'sandybrown', 'palegreen', 'lightslategrey',\n",
        "            'teal', 'red', 'dodgerblue', 'darkslategrey', 'mediumslateblue',\n",
        "            'orangered', 'springgreen', 'mediumaquamarine', 'darkgrey', 'firebrick',\n",
        "            'salmon', 'darkkhaki', 'yellowgreen', 'forestgreen', 'cadetblue',\n",
        "            'plum', 'seagreen', 'royalblue', 'goldenrod', 'slateblue', 'fuchsia',\n",
        "            'khaki', 'chocolate', 'hotpink', 'moccasin', 'crimson', 'peachpuff'\n",
        "        ])\n",
        "        color_iter_one = itertools.islice(color_iter, 1, None)\n",
        "        plot_results(train_new[col_names].values, train_new['cluster_labels'].values, nclus, 2, plot_title + ' no unclustered')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(tsne_plot, dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    if dims == 3:\n",
        "        ET = EllipsoidTool()\n",
        "        fig = plt.figure(figsize=(10, 8))\n",
        "        if black_bkg:\n",
        "            plt.style.use('dark_background')\n",
        "        color_iter = itertools.cycle([\n",
        "            'orchid', 'cyan', 'violet', 'lawngreen', 'cornflowerblue', 'tomato',\n",
        "            'gold', 'deeppink', 'lime', 'darkorange', 'olive', 'lightcoral',\n",
        "            'tan', 'sandybrown', 'palegreen', 'lightslategrey',\n",
        "            'teal', 'red', 'dodgerblue', 'darkslategrey', 'mediumslateblue',\n",
        "            'orangered', 'springgreen', 'mediumaquamarine', 'darkgrey', 'firebrick',\n",
        "            'salmon', 'darkkhaki', 'yellowgreen', 'forestgreen', 'cadetblue',\n",
        "            'plum', 'seagreen', 'royalblue', 'goldenrod', 'slateblue', 'fuchsia',\n",
        "            'khaki', 'chocolate', 'hotpink', 'moccasin', 'crimson', 'peachpuff'\n",
        "        ])\n",
        "        color_iter_one = itertools.islice(color_iter, 1, None)\n",
        "\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.set_aspect('equal')\n",
        "        ax.set_xlabel('Dimension 1')\n",
        "        ax.set_ylabel('Dimension 2')\n",
        "        ax.set_zlabel('Dimension 3')\n",
        "\n",
        "        unique_clusters = np.arange(nclus).astype(np.int)\n",
        "        for i, (zz, color) in enumerate(zip(unique_clusters, color_iter_one)):\n",
        "            ax.scatter(train.loc[train.cluster_labels == zz, col_names_X],\n",
        "                       train.loc[train.cluster_labels == zz, col_names_Y],\n",
        "                       train.loc[train.cluster_labels == zz, col_names_Z],\n",
        "                       marker='.',\n",
        "                       s=point_size,\n",
        "                       c=color)\n",
        "\n",
        "            dp = train.loc[train.cluster_labels == zz]\n",
        "            a_points = dp[col_names].values\n",
        "            # find the ellipsoid\n",
        "            (center, radii, rotation) = ET.getMinVolEllipse(a_points, .001)\n",
        "            # plot the ellipsoid\n",
        "            ET.plotEllipsoid(center,\n",
        "                             radii,\n",
        "                             rotation,\n",
        "                             ax=ax,\n",
        "                             plotAxes=False,\n",
        "                             cageColor=color,\n",
        "                             cageAlpha=0.1)\n",
        "            xc, yc, zc = center\n",
        "            ax.text(xc, yc, zc, str(zz), 'y', fontsize=12)\n",
        "\n",
        "        plt.title(plot_title, fontsize='16')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(tsne_plot, dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    if verbose:\n",
        "        print('\\n tSNE plot saved as %s' % tsne_plot)\n",
        "\n",
        "    train_index = pd.read_csv('contigs_%s_%s.%dmer_index.csv' %\n",
        "                              (hc_abbrev, lc_abbrev, kmer_size))\n",
        "    train_index['cluster_labels'] = clabels.astype(np.int)\n",
        "    train_index.columns = ['contig_id', 'cluster_id']\n",
        "    train_index.to_csv('%dD_' % dims + tsne_clusters, index=False)\n",
        "    if verbose:\n",
        "        print('\\n Extracting clusters from %s\\n' %\n",
        "              ('%dD_' % dims + tsne_clusters))\n",
        "    os.system('merge_cutup_clustering.py %s > merged_%s' %\n",
        "              (('%dD_' % dims + tsne_clusters),\n",
        "               ('%dD_' % dims + tsne_clusters)))\n",
        "    os.system('extract_fasta_bins.py %s merged_%s --output_path %s' %\n",
        "              (input_file,\n",
        "               ('%dD_' % dims + tsne_clusters), bins))\n",
        "    if os.path.exists('%s/group_-1.fa' % bins):\n",
        "        os.system('mv -- %s/group_-1.fa %s/unclustered.fasta' % (bins, bins))\n",
        "\n",
        "# remove tmp files\n",
        "    if os.access('data.dat', os.R_OK):\n",
        "        os.system('rm data.dat')\n",
        "    if os.access('result.dat', os.R_OK):\n",
        "        os.system('rm result.dat')\n",
        "    if os.access('tmp.csv', os.R_OK):\n",
        "        os.system('rm tmp.csv')\n",
        "    if os.access('contigs_%s.fasta' % hc_abbrev, os.R_OK):\n",
        "        os.system('rm contigs_%s.fasta' % hc_abbrev)\n",
        "    if os.access('contigs_%s_%s.fasta' % (hc_abbrev, lc_abbrev), os.R_OK):\n",
        "        os.system('rm contigs_%s_%s.fasta' % (hc_abbrev, lc_abbrev))\n",
        "    if os.access(\n",
        "            'contigs_%s_%s.%dmer.tab' % (hc_abbrev, lc_abbrev, kmer_size),\n",
        "            os.R_OK):\n",
        "        os.system('rm contigs_%s_%s.%dmer.tab' %\n",
        "                  (hc_abbrev, lc_abbrev, kmer_size))\n",
        "    if os.access(\n",
        "            'contigs_%s_%s.%dmer.csv' % (hc_abbrev, lc_abbrev, kmer_size),\n",
        "            os.R_OK):\n",
        "        os.system('rm contigs_%s_%s.%dmer.csv' %\n",
        "                  (hc_abbrev, lc_abbrev, kmer_size))\n",
        "    if os.access(\n",
        "            'contigs_%s_%s.%dmer_index.csv' %\n",
        "        (hc_abbrev, lc_abbrev, kmer_size), os.R_OK):\n",
        "        os.system('rm contigs_%s_%s.%dmer_index.csv' %\n",
        "                  (hc_abbrev, lc_abbrev, kmer_size))\n",
        "    if os.access(\n",
        "            'contigs_%s_%s.%dmer_noindex.csv' %\n",
        "        (hc_abbrev, lc_abbrev, kmer_size), os.R_OK):\n",
        "        os.system('rm contigs_%s_%s.%dmer_noindex.csv' %\n",
        "                  (hc_abbrev, lc_abbrev, kmer_size))\n",
        "\n",
        "else:\n",
        "    parser.error(\n",
        "        '\\n !!! Input file \"%s\" does not exist in this directory !!!\\n' %\n",
        "        input_file)\n"
      ],
      "metadata": {
        "id": "C43q6au2Gbqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRdwP7Y2F1rD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}